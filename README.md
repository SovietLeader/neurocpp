# NeuroCPP â€” Modular, Interpretable Neural Networks in C++

**From scratch. With understanding. For CPU first, GPU later.**

A modern C++ deep learning framework focused on transparency, modularity, and interpretability. Built as a learning vehicle to understand neural networks from the ground up.

---

## âœ… Current Status

**Scalar Autodiff Core Complete** featuring:

- Tape-based computation graph
- Full binary and unary operations support
- Analytically verified gradients
- Clean computation/differentiation separation

---

## ðŸš€ Quick Examples
**Basic Scalar Autodiff**  
*Build computation graphs intuitively with operator overloading (work-in-progress API):*
```cpp
#include "neurocpp/autograd.hpp"

int main() {
    // Input variables (under active refactoring to Var API)
    auto x = input(2.0);  // Current: mgr.input_node(2.0)
    auto y = input(3.0);
    
    // Expressions with natural syntax
    auto z = (x + y) * sin(x);  // Tape records: add â†’ sin â†’ mul
    
    backward(z);  // Propagate gradients backward
    
    std::cout << "dz/dx = " << grad(x) << "\n";  // â‰ˆ -1.1712
    std::cout << "dz/dy = " << grad(y) << "\n";  // â‰ˆ 0.9093
}
```
**Gradient Based Optimization**
```cpp
// Minimize f(x) = (x - 3)^2 using gradients
auto x = input(0.0);  // Start at x=0
for (int i = 0; i < 10; ++i) {
    auto loss = square(x - 3.0);  // (x-3)^2
    backward(loss);
    
    // Update: x = x - Î· * âˆ‚loss/âˆ‚x
    x.value() -= 0.1 * grad(x);  // Î·=0.1 (learning rate)
    
    std::cout << "x = " << x.value() << " | loss = " << loss.value() << "\n";
}
/* Output:
x = 0.6 | loss = 5.76
x = 1.08 | loss = 3.6864
... converges to x=3.0 */
```
**Custom Derivative Calculation**
```cpp
// Test: âˆ‚/âˆ‚x [xÂ² * sin(x)] at x=1.0
auto x = input(1.0);
auto z = square(x) * sin(x);

backward(z);
double numerical = grad(x);  // NeuroCPP's result

// Analytical derivative: 2xÂ·sin(x) + xÂ²Â·cos(x)
double analytical = 2*1.0*sin(1.0) + 1.0*1.0*cos(1.0);

assert(std::abs(numerical - analytical) < 1e-6);  // âœ… Passes!
```
---

## ðŸƒ Roadmap

| Phase | Status | Timeline |
|-------|--------|----------|
| **Scalar Autodiff** | âœ… Complete | Core stable |
| **Tensor Engine** | ðŸ”„ In Progress | Next major milestone |
| **Neural Layers** | â³ Planned | 2025 |
| **Recurrent & Attention** | â³ Planned | 2025 |
| **Visualization Tools** | â³ Planned | 2025 |
| **CPU Optimizations** | â³ Planned | 2026 |
| **OpenGL Backend** | â³ Planned | 2026 |

---

## ðŸŽ¯ Key Features

- **Transparent**: Every operation inspectable, every gradient traceable
- **Modular**: Build any architecture by wiring simple layers
- **Interpretable**: Heatmaps for activation and gradient flow
- **Portable**: Future GPU acceleration via OpenGL

---

## ðŸ›  Building

*Basic compilation example:*
Ð’Ð¡Ð¢ÐÐ’Ð¬Ð¢Ð• ÐŸÐ Ð˜ÐœÐ•Ð  ÐšÐžÐœÐŸÐ˜Ð›Ð¯Ð¦Ð˜Ð˜ Ð—Ð”Ð•Ð¡Ð¬

---

## ðŸ’¡ Philosophy

- **Learn by building**: No black boxes
- **CPU-first**: Optimize for available hardware
- **Interpretability over convenience**: Understanding trumps speed
- **Minimal dependencies**: Standard C++ only

---

## ðŸ¤ Contributing

Personal educational project â€” feedback and collaboration welcome!

**Author**: Vladimir Ryzhkov  
**License**: MIT (planned)
